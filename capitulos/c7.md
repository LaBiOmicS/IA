# Capítulo 7 – Máquinas de Vetores de Suporte (SVM)

## 7.0 Visão geral e motivação
As **Máquinas de Vetores de Suporte (Support Vector Machines, SVM)** foram popularizadas por Vapnik e colaboradores nos anos 1990 como um método **convexo** e **bem fundamentado** para classificação (e também regressão, via SVR). SVMs:
- Funcionam muito bem em **alta dimensionalidade** (p.ex., perfis de **expressão gênica** com milhares de features).
- Possuem uma teoria de **margem máxima** que conecta **capacidade do modelo** e **generalização** (idéia de “**structural risk minimization**”).
- Oferecem separação **não linear** via **kernel trick**, sem computar explicitamente a transformação para espaços de dimensão alta (ou infinita).

Aplicações biomédicas típicas:
- Classificar tumores como **benignos/malignos** a partir de omics (n ≪ d).
- Predizer **risco** (p.ex., diabetes) a partir de variáveis clínicas.
- Identificar **subtipos** moleculares em dados de expressão/transcriptoma.

---

## 7.1 Geometria: hiperplano, margem e vetores de suporte
Considere dados rotulados \((x_i, y_i)\), com \(x_i \in \mathbb{R}^d\) e \(y_i \in \{-1, +1\}\).

Um **hiperplano** é dado por \(f(x)=w^\top x + b\). A **margem geométrica** é a distância entre o hiperplano e os pontos de cada classe mais próximos a ele. A SVM busca o hiperplano que **maximiza a margem**. Os pontos que tocam as “margens” são os **vetores de suporte**; somente eles determinam a solução ótima.

Intuição: margens largas tendem a generalizar melhor, reduzindo o risco de overfitting.

---

## 7.2 Formulação primal (hard-margin, linearmente separável)
Quando os dados são separáveis sem erro:

\[
\begin{aligned}
\min_{w,b}\quad & \frac{1}{2}\lVert w\rVert^2 \\
\text{s.a.}\quad & y_i\,(w^\top x_i + b) \ge 1,\quad i=1,\dots,n
\end{aligned}
\]

- Minimizar \(\tfrac{1}{2}\lVert w\rVert^2\) **maximiza a margem** \(\frac{2}{\lVert w\rVert}\).
- Restrições garantem que todos os pontos estão do **lado correto** da margem.

---

## 7.3 Soft-margin: violando margens com variáveis de folga
Dados reais têm ruído e sobreposição entre classes. Introduzimos \(\xi_i \ge 0\):

\[
\begin{aligned}
\min_{w,b,\xi}\quad & \frac{1}{2}\lVert w\rVert^2 + C\sum_{i=1}^{n}\xi_i \\
\text{s.a.}\quad & y_i\,(w^\top x_i + b) \ge 1 - \xi_i,\quad \xi_i \ge 0
\end{aligned}
\]

- **C** controla o *trade-off* entre **margem larga** (pequeno \(\lVert w\rVert\)) e **penalidade por violações** (\(\sum \xi_i\)).
  - **C grande**: modelo “duro”, permite poucos erros (tende a menor viés, maior variância).
  - **C pequeno**: modelo mais “flexível” e tolerante a erros (maior viés, menor variância).

### 7.3.1 Perda hinge e ERM (visão de aprendizado)
A formulação soft-margin equivale (até constantes) a minimizar **perda hinge** regularizada:

\[
\min_{w,b}\ \frac{1}{2}\lVert w\rVert^2 + C \sum_{i=1}^n \max\{0, 1 - y_i(w^\top x_i + b)\}.
\]

Isso é **empirical risk minimization** com regularização L2: a primeira parcela controla a **complexidade** (norma de \(w\)); a segunda, o **erro empírico** (hinge).

---

## 7.4 Formulação dual, multiplicadores de Lagrange e suporte
O **dual** revela estrutura e viabiliza o **kernel trick**:

\[
\begin{aligned}
\max_{\alpha}\quad & \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j (x_i^\top x_j) \\
\text{s.a.}\quad & 0 \le \alpha_i \le C,\quad \sum_{i=1}^n \alpha_i y_i=0
\end{aligned}
\]

A solução tem a forma:
\[
w = \sum_{i=1}^n \alpha_i y_i x_i.
\]

Apenas amostras com \(\alpha_i>0\) contribuem: **vetores de suporte**. O viés \(b\) é recuperado a partir das restrições ativas (pontos no “margem”).

---

## 7.5 Condições KKT (ótimo de primeira ordem para problemas convexos)
Para cada \(i\):

- **Primal factível:** \(y_i(w^\top x_i + b) \ge 1 - \xi_i,\ \xi_i \ge 0.\)
- **Dual factível:** \(0 \le \alpha_i \le C,\ \mu_i \ge 0.\)
- **Complementaridade:** \(\alpha_i[1 - \xi_i - y_i(w^\top x_i + b)] = 0,\ \mu_i \xi_i=0.\)
- **Estacionaridade:** \(w = \sum_i \alpha_i y_i x_i.\)

Interpretação: pontos **bem dentro** da margem têm \(\alpha_i=0\); **sobre a margem** têm \(0<\alpha_i<C\); **mal classificados**/dentro da margem tendem a \(\alpha_i=C\).

---

## 7.6 Kernel trick e espaços de Hilbert reproduzíveis (RKHS)
Substitua produtos internos \(x_i^\top x_j\) por um **kernel** \(K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle\). É suficiente que \(K\) seja **simétrico** e **semi-definido positivo** (teorema de Mercer).

Kernels comuns:
- **Linear:** \(K(x,z)=x^\top z\) — rápido, ótimo para n≪d (omics).
- **Polinomial:** \(K(x,z)=(\gamma x^\top z + r)^d\) — interações.
- **RBF (gaussiano):** \(K(x,z)=\exp(-\gamma\lVert x-z\rVert^2)\) — muito flexível.
- **Sigmoid:** \(K(x,z)=\tanh(\gamma x^\top z + r)\) — similar a MLP em certos regimes.

### Hiperparâmetros do kernel
- **RBF:** \(\gamma\) controla “alcance” local de cada amostra (grande \(\gamma\) → fronteiras muito onduladas). Heurística inicial: \(\gamma \approx \tfrac{1}{d \,\mathrm{Var}(X)}\) (ajuste por validação).
- **Polinomial:** grau \(d\), \(\gamma\) e \(r\) influenciam complexidade.

**Cuidado:** kernels custam \(O(n^2)\) memória/tempo para a **matriz de kernel**. Para **n** grande, prefira **linear** (LinearSVC, SGDClassifier com `loss="hinge"`).

---

## 7.7 Otimização na prática e complexidade
- **QP genérico**: resolvido por resolvedores quadráticos (caro para n grande).
- **SMO (Sequential Minimal Optimization)**: otimiza pares \((\alpha_i,\alpha_j)\) iterativamente; base do **LIBSVM** (SVC).
- **Coordinate Descent / LIBLINEAR**: para **linear SVM** (muito rápido em alta dimensionalidade).
- **Complexidade**:
  - Kernel SVM: memória \(O(n^2)\) e tempo entre \(O(n^2)\) e \(O(n^3)\) (dependente).
  - Linear SVM: escalável; tempo quase linear em \(\#\)não-zeros da matriz \(X\).

---

## 7.8 Multiclasse, probabilidades e calibração
SVMs são intrinsicamente binárias. Estratégias:
- **One-vs-Rest (OvR):** treina \(K\) classificadores binários para \(K\) classes.
- **One-vs-One (OvO):** treina \(\tfrac{K(K-1)}{2}\) classificadores; voto por maioria (padrão no `SVC`).

**Probabilidades:** SVM não é probabilística. Para obter probabilidades calibradas:
- **Platt scaling** (sigmoidal) ou **Isotonic Regression** via `CalibratedClassifierCV`.
- Avalie **calibração** (Brier score / reliability plots) quando decisões clínicas dependem de thresholds.

---

## 7.9 Pré-processamento, vazamento e seleção de features
- **Padronização é crucial** (média 0, desvio 1) → `StandardScaler`.
- **Pipeline** (`sklearn.pipeline.Pipeline`) evita **vazamento** (data leakage): *fit* de scaler e seleção só no **treino** dentro da CV.
- **Seleção de features** (n≪d):
  - `SelectKBest` (ex.: `mutual_info_classif`, `f_classif`) ou **L1** (para linear).
  - **Sempre dentro do pipeline** para não “espiar” o teste.

---

## 7.10 Classes desbalanceadas e custo sensível
- Use `class_weight="balanced"` ou pesos customizados.
- Otimize **métricas apropriadas**: ROC-AUC, **PR-AUC** (quando positivos são raros).
- Ajuste **limiar** de decisão (threshold) para maximizar F1/Recall conforme o custo clínico.

---

## 7.11 Interpretação
- **Linear SVM**: coeficientes \(w\) dão **importância direcional** (após padronização).
- **Kernel SVM (RBF)**: menos interpretável; use **importância por permutação**, **PDP/ICE** (com cautela) ou **SHAP** (mais caro).

---

## 7.12 Exemplos completos (Python)
### 7.12.1 Pipeline OvO (padrão), padronização e busca de hiperparâmetros (Iris)
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

iris = load_iris()
X, y = iris.data, iris.target

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC())
])

param_grid = {
    "svm__kernel": ["linear", "rbf"],
    "svm__C": [0.1, 1, 10],
    "svm__gamma": ["scale", 0.01, 0.1]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
gs = GridSearchCV(pipe, param_grid, cv=cv, scoring="f1_macro", n_jobs=-1)
gs.fit(X_tr, y_tr)

print("Melhores parâmetros:", gs.best_params_)
y_pred = gs.predict(X_te)
print(confusion_matrix(y_te, y_pred))
print(classification_report(y_te, y_pred, target_names=iris.target_names))
```

### 7.12.2 Alto-dimensional (n≪d): linear SVM com seleção de features
```python
import numpy as np
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_val_score

rng = np.random.default_rng(42)
n, d = 80, 5000
X = rng.normal(size=(n, d))
y = rng.integers(0, 2, size=n)

pipe = Pipeline([
    ("scaler", StandardScaler(with_mean=False)),
    ("sel", SelectKBest(mutual_info_classif, k=300)),
    ("clf", LinearSVC(C=1.0))
])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(pipe, X, y, cv=cv, scoring="f1")
print("F1 médio (5-fold):", scores.mean())
```

### 7.12.3 Desequilíbrio + probabilidades calibradas
```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, brier_score_loss
from sklearn.calibration import CalibratedClassifierCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

X, y = make_classification(n_samples=3000, n_features=20, weights=[0.9, 0.1],
                           class_sep=1.0, random_state=42)

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

base = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(kernel="rbf", C=1.0, gamma="scale", class_weight="balanced", probability=True))
])

cal = CalibratedClassifierCV(base, cv=3)
cal.fit(X_tr, y_tr)
proba = cal.predict_proba(X_te)[:, 1]

y_pred = (proba >= 0.5).astype(int)
print(classification_report(y_te, y_pred, digits=3))
print("Brier score:", brier_score_loss(y_te, proba))
```

---

## 7.13 Exercícios
1. Deduzir a formulação dual da SVM soft-margin a partir do Lagrangiano.
2. Explique geometricamente o papel dos vetores de suporte.
3. Mostre que margens largas implicam menor norma de w.
4. Reproduza em Python o problema soft-margin via `cvxopt`.
5. Compare kernels linear, RBF e polinomial em Iris.
6. Use validação cruzada para escolher C e gamma.
7. Simule um dataset clínico e aplique SVM.
8. Estude impacto do desbalanceamento (10% positivos) com e sem class_weight.
9. Calibre probabilidades e avalie Brier score.
10. Compare interpretabilidade: coeficientes da linear SVM vs feature importance de RF.

---

## 7.14 Conclusão
SVMs conectam **geometria**, **otimização convexa** e **boa prática estatística**. Para problemas biomédicos de **alta dimensionalidade** e **baixa amostragem**, **linear SVM** com **padronização** e **seleção de features** em **pipeline** é um forte ponto de partida. Para relações não lineares, **RBF** é prática, mas valide e calibre conforme contexto clínico.

---
