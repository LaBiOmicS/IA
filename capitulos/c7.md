# Capítulo 7 – Máquinas de Vetores de Suporte (SVM)

## 7.0 Visão geral e motivação
As Máquinas de Vetores de Suporte (Support Vector Machines, SVM) foram popularizadas por Vapnik e colegas nos anos 1990 como um método **convexo** e **bem fundamentado** para classificação (e também regressão, via SVR).  

Por que usar SVM?
- Funciona bem em **alta dimensionalidade** (ex.: perfis de expressão gênica com milhares de variáveis).  
- Baseia-se na ideia de **margem máxima**, conectando geometria e generalização.  
- Suporta **separação não linear** via kernel trick.  

Aplicações em bioinformática e saúde:
- Classificação de tumores como benignos/malignos.  
- Predição de risco (ex.: diabetes) a partir de variáveis clínicas.  
- Identificação de subtipos moleculares em transcriptomas.  

---

## 7.1 Geometria: hiperplano, margem e vetores de suporte
Considere pares de treino (x, y), com x sendo um vetor de atributos e y ∈ {−1, +1}.  

Um **hiperplano** é dado por:

```
f(x) = w · x + b
```

- `w` é o vetor de pesos (normal ao hiperplano).  
- `b` é o viés (bias).  

A SVM busca o **hiperplano que maximiza a margem** entre as classes.  
Os pontos mais próximos ao hiperplano são os **vetores de suporte** e definem a fronteira de decisão.  

---

## 7.2 Formulação primal (hard-margin, separável)
Se os dados forem linearmente separáveis:

```
Minimizar:  (1/2) ||w||²
Sujeito a:  y_i (w · x_i + b) >= 1, para todo i
```

Ou seja:
- Maximizar a margem equivale a minimizar a norma de `w`.  
- Todos os pontos precisam estar corretamente classificados e fora da margem.  

---

## 7.3 Soft-margin (dados com sobreposição)
Na prática, dados reais têm ruído. Permitimos violações com variáveis ξ (xi):

```
Minimizar:  (1/2) ||w||² + C Σ ξ_i
Sujeito a:  y_i (w · x_i + b) >= 1 - ξ_i   e   ξ_i >= 0
```

- **C** controla o equilíbrio entre margem larga e erros permitidos.  
- C grande → menos erros, margem pequena.  
- C pequeno → mais erros tolerados, margem maior.  

---

## 7.4 Perda hinge
O problema acima é equivalente a minimizar a **perda hinge** regularizada:

```
L(w, b) = (1/2) ||w||² + C Σ max(0, 1 - y_i (w · x_i + b))
```

- O primeiro termo controla a complexidade do modelo.  
- O segundo termo penaliza erros ou pontos dentro da margem.  

---

## 7.5 Formulação dual e vetores de suporte
O problema dual depende apenas de produtos internos (x_i · x_j), o que habilita o uso de kernels.  

A solução ótima é uma combinação linear de amostras de treino:

```
w = Σ α_i y_i x_i
```

Apenas amostras com α_i > 0 se tornam **vetores de suporte**.  

---

## 7.6 Kernel trick
O truque do kernel substitui o produto interno por uma função de similaridade:

- **Linear:** K(x, z) = x · z  
- **Polinomial:** K(x, z) = (γ x · z + r)^d  
- **RBF (gaussiano):** K(x, z) = exp(−γ ||x − z||²)  
- **Sigmoid:** K(x, z) = tanh(γ x · z + r)  

Assim, podemos separar dados não lineares em espaços de maior dimensão sem calculá-los explicitamente.  

---

## 7.7 Multiclasse e probabilidades
- **Multiclasse:** implementado via estratégias One-vs-Rest (OvR) ou One-vs-One (OvO).  
- **Probabilidades:** a saída da SVM não é probabilística. Para calibrar, usamos métodos como Platt Scaling ou Isotonic Regression (`CalibratedClassifierCV` no scikit-learn).  

---

## 7.8 Exemplo prático com Iris
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.3, random_state=42
)

model = SVC(kernel="rbf", C=1, gamma="scale")
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

---

## 7.9 Aplicações biomédicas
- Classificação de tumores (ex.: usando expressão gênica).  
- Predição de risco de doenças crônicas (idade, IMC, colesterol, glicemia).  
- Diferenciação de microbiomas saudáveis vs. doentes.  

---

## 7.10 Boas práticas
- **Escalonar os dados** (StandardScaler) antes da SVM.  
- Usar **validação cruzada** para escolher C e γ.  
- Avaliar métricas apropriadas para dados desbalanceados (ex.: AUC-ROC, PR-AUC).  

---

## 7.11 Exercícios

1. Treine uma SVM linear e compare com RBF no dataset Iris.  
2. Simule um dataset clínico com idade, IMC, glicemia e colesterol, e use SVM para classificar risco.  
3. Varie o hiperparâmetro **C** e observe a mudança na fronteira de decisão.  
4. Varie o hiperparâmetro **gamma** (kernel RBF) e explique o efeito.  
5. Use validação cruzada para encontrar melhores hiperparâmetros.  
6. Aplique SVM em um dataset público de expressão gênica.  
7. Compare SVM com Random Forests.  
8. Analise desempenho em dados desbalanceados, com e sem `class_weight="balanced"`.  
9. Calibre as probabilidades e avalie o Brier Score.  
10. Escreva um relatório comparando cenários em que SVM é mais apropriada que árvores de decisão.  

---

## 7.12 Conclusão
SVM é um modelo elegante e poderoso, especialmente em problemas de **alta dimensionalidade e baixo número de amostras**. Apesar de menos interpretável que árvores, sua robustez e flexibilidade com kernels a tornam muito útil em **bioinformática e saúde**.

---
