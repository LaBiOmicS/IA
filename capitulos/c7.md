# Capítulo 7 – Máquinas de Vetores de Suporte (SVM)

## 7.0 Visão geral e motivação
As Máquinas de Vetores de Suporte (Support Vector Machines, SVM) foram popularizadas nos anos 1990 como um dos algoritmos mais robustos e teóricos do aprendizado de máquina. Elas são muito utilizadas em **bioinformática** e **informática biomédica**, especialmente em dados de alta dimensionalidade (ex.: milhares de genes para poucas amostras).

---

## 7.1 Geometria e ideia central
Uma SVM busca um **hiperplano** que separe duas classes de dados com a maior margem possível.  
Os pontos que ficam mais próximos da margem são os **vetores de suporte** e definem a solução ótima.

---

## 7.2 Formulações matemática (resumida)
- **Hard-Margin:** para dados separáveis sem erro.  
- **Soft-Margin:** adiciona tolerância a erros, controlada pelo hiperparâmetro **C**.  
- **Kernel Trick:** permite separar dados não lineares transformando-os em espaços de maior dimensão implicitamente.

---

## 7.3 SVM no scikit-learn
O `scikit-learn` implementa SVMs através de duas principais classes:

- **`sklearn.svm.SVC`** – Classificação com kernels (linear, polinomial, RBF, sigmoide).  
- **`sklearn.svm.LinearSVC`** – Implementação otimizada para problemas lineares de grande escala (usa liblinear).  
- **`sklearn.svm.SVR`** – Versão para regressão (não abordada aqui).  

### Diferença prática
- Use `SVC` quando deseja experimentar diferentes kernels (ex.: RBF, polinomial).  
- Use `LinearSVC` quando o número de amostras ou features é muito grande (omics, texto biomédico).  

---

## 7.4 Principais hiperparâmetros

### 7.4.1 C (penalidade por erro)
- Controla o equilíbrio entre margem larga e erro de classificação.  
- **C alto** → modelo mais rígido, menos erros no treino, mas risco de overfitting.  
- **C baixo** → modelo mais flexível, aceita erros, maior generalização.

### 7.4.2 Kernel
Define a função de transformação dos dados:  
- `"linear"` – separação linear.  
- `"poly"` – polinomial.  
- `"rbf"` – radial basis function (mais usado em dados biomédicos).  
- `"sigmoid"` – semelhante a redes neurais.

### 7.4.3 Gamma (γ)
- Específico para kernels RBF e polinomial.  
- Controla o alcance da influência de cada ponto.  
- **Gamma alto** → fronteiras complexas (overfitting).  
- **Gamma baixo** → fronteiras suaves (underfitting).

### 7.4.4 Degree (grau do polinômio)
- Só usado no kernel `"poly"`.  
- Define a complexidade da fronteira.

### 7.4.5 class_weight
- Permite lidar com dados desbalanceados.  
- `"balanced"` ajusta pesos inversamente proporcionais à frequência de cada classe.  
- Essencial em cenários biomédicos (ex.: 90% saudáveis, 10% doentes).

### 7.4.6 probability
- Ativa a estimação de probabilidades (`predict_proba`).  
- Usa calibração interna (Platt Scaling).  
- Útil em contextos clínicos (predição de risco).

---

## 7.5 Exemplo prático com Iris
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.3, stratify=iris.target, random_state=42
)

# Pipeline: padronização + SVM
pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(kernel="rbf", C=1, gamma="scale", probability=True))
])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

---

## 7.6 Ajuste de hiperparâmetros
O desempenho da SVM depende fortemente de **C** e **gamma**.  
Podemos usar validação cruzada com Grid Search:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    "svm__C": [0.1, 1, 10],
    "svm__gamma": ["scale", 0.01, 0.1, 1],
    "svm__kernel": ["rbf", "linear"]
}

grid = GridSearchCV(pipe, param_grid, cv=5, scoring="f1_macro", n_jobs=-1)
grid.fit(X_train, y_train)

print("Melhores parâmetros:", grid.best_params_)
print("Acurácia no teste:", grid.score(X_test, y_test))
```

---

## 7.7 Aplicações biomédicas com SVM
- **Genômica/transcriptômica:** classificação de tumores com expressão gênica.  
- **Microbioma:** diferenciação entre microbiomas saudáveis e doentes.  
- **Dados clínicos tabulares:** previsão de risco cardiovascular usando variáveis como pressão, colesterol e IMC.  

---

## 7.8 Boas práticas no uso de SVM
1. **Sempre escalar os dados** (StandardScaler ou MinMaxScaler).  
2. Usar **pipelines** para evitar vazamento de dados.  
3. Ajustar hiperparâmetros via **validação cruzada**.  
4. Usar **class_weight="balanced"** em casos desbalanceados.  
5. Calibrar probabilidades quando decisões clínicas dependem de thresholds.  

---

## 7.9 Exercícios

1. Treine uma `LinearSVC` no dataset Iris. Compare com `SVC(kernel="linear")`.  
2. Use `SVC(kernel="rbf")` em um dataset clínico simulado. Varie C e gamma.  
3. Teste o kernel `"poly"` com diferentes graus.  
4. Use `class_weight="balanced"` em dados desbalanceados e compare métricas.  
5. Ative `probability=True` e use `predict_proba` para estimar riscos.  
6. Monte um pipeline com `StandardScaler` + SVM e use `GridSearchCV`.  
7. Compare SVM com Árvores de Decisão no mesmo dataset.  
8. Treine SVM em um dataset de expressão gênica público e discuta resultados.  
9. Faça gráficos de fronteiras de decisão em 2D.  
10. Escreva um relatório discutindo prós e contras de SVM em aplicações biomédicas.  

---

## 7.10 Conclusão
O `scikit-learn` torna a aplicação de SVM acessível e poderosa.  
O domínio dos **hiperparâmetros** (C, gamma, kernel, class_weight) e boas práticas (normalização, validação cruzada) é essencial para usar o modelo de forma eficaz em **bioinformática e saúde**.

---
