# Cap√≠tulo 6 ‚Äì Aprendizado Supervisionado: Classifica√ß√£o

## 6.0 Introdu√ß√£o
O **aprendizado supervisionado** √© o tipo de Machine Learning onde o modelo √© treinado com **exemplos rotulados**.  
Ou seja, temos um conjunto de **entradas (features)** e suas **sa√≠das esperadas (labels)**, e o algoritmo aprende a **mapear entradas para sa√≠das**.

Na **classifica√ß√£o**, o objetivo √© prever **categorias discretas**.  
Exemplos biom√©dicos:
- Diagn√≥stico de um exame: **positivo/negativo**.  
- Classifica√ß√£o de tumores: **benigno/maligno**.  
- Identifica√ß√£o de bact√©rias por sequenciamento: **esp√©cies distintas**.  

O fluxo √© sempre o mesmo:
1. Obter e preparar os dados.  
2. Dividir em treino e teste.  
3. Escolher o modelo.  
4. Treinar o modelo (`fit`).  
5. Fazer predi√ß√µes (`predict`).  
6. Avaliar com m√©tricas adequadas.  

Neste cap√≠tulo estudaremos tr√™s modelos cl√°ssicos de classifica√ß√£o:
- **SVM (Support Vector Machines)**  
- **√Årvores de Decis√£o**  
- **Random Forests**  

---

## 6.1 Dataset Iris como exemplo
O **dataset Iris** √© o ‚Äúol√° mundo‚Äù da classifica√ß√£o.  
Ele cont√©m medidas de p√©talas e s√©palas de 150 flores de tr√™s esp√©cies: *setosa*, *versicolor* e *virginica*.  

```python
from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris(as_frame=True)
df = iris.frame
print(df.head())
```

Este dataset ser√° usado para treinar nossos modelos.

---

## 6.2 M√°quinas de Vetores de Suporte (SVM)

### Conceito
As **M√°quinas de Vetores de Suporte (Support Vector Machines, SVM)** s√£o algoritmos de classifica√ß√£o supervisionada que buscam um **hiperplano √≥timo** para separar classes no espa√ßo das features.  

- Em 2D, o hiperplano √© uma **reta**; em 3D, um **plano**; em dimens√µes maiores, uma **hipersuperf√≠cie**.  
- O objetivo √© **maximizar a margem**, ou seja, a dist√¢ncia entre o hiperplano e os exemplos mais pr√≥ximos de cada classe. Esses exemplos s√£o chamados de **vetores de suporte**.

Se os dados n√£o forem linearmente separ√°veis, usamos **kernels**:
- **Linear** ‚Üí adequado quando as classes j√° se separam bem em linha reta.  
- **Polinomial** ‚Üí cria fronteiras mais curvas, √∫til quando h√° intera√ß√µes n√£o lineares.  
- **RBF (Radial Basis Function)** ‚Üí projeta os dados em um espa√ßo de dimens√£o muito maior, separando classes complexas.  

üî¨ **Exemplo biom√©dico**:  
Imagine que queremos separar pacientes com risco de diabetes de baixo e alto risco usando **IMC e glicemia**. Se uma linha n√£o for suficiente, o kernel RBF permite criar uma **curva n√£o linear** que separa melhor os grupos.

### C√≥digo ‚Äî SVM no Iris
```python
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.3, random_state=42)

svm = SVC(kernel="rbf", C=1, gamma="scale")
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)

print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

### Vantagens
- Funciona bem em **alta dimensionalidade**.  
- Robusta a overfitting quando bem regularizada.  

### Limita√ß√µes
- Pouco interpret√°vel.  
- Treinamento lento em grandes datasets.  

---

## 6.3 √Årvores de Decis√£o

### Conceito
Uma **√Årvore de Decis√£o** funciona como um **fluxo de perguntas e respostas**, parecido com um question√°rio m√©dico.  
- Cada **n√≥ interno** representa uma pergunta (ex.: ‚ÄúA glicemia √© maior que 126 mg/dL?‚Äù).  
- Cada **ramo** representa uma resposta (‚ÄúSim‚Äù ou ‚ÄúN√£o‚Äù).  
- Cada **folha** representa uma classe final (ex.: ‚ÄúPaciente em risco‚Äù ou ‚ÄúPaciente saud√°vel‚Äù).

O treinamento consiste em escolher, em cada divis√£o, a **melhor feature** para separar as classes, usando medidas como:  
- **Gini** (impureza).  
- **Entropia** (informa√ß√£o).  

O processo continua at√© atingir profundidade m√°xima ou at√© que os n√≥s n√£o tragam ganho de informa√ß√£o.  

üí° **Problema:** √°rvores tendem a **overfitting**, ou seja, podem decorar os dados de treino. Para evitar isso, aplicamos **poda (pruning)**, limitando a profundidade ou o n√∫mero m√≠nimo de amostras por n√≥.

üî¨ **Exemplo biom√©dico**:  
Um m√©dico poderia usar uma √°rvore para prever risco de hipertens√£o:  
1. Perguntar pela idade (> 50 anos?).  
2. Perguntar pela press√£o sist√≥lica (> 140 mmHg?).  
3. Perguntar pelo IMC (> 30?).  
Cada combina√ß√£o leva a uma classifica√ß√£o final.

### C√≥digo ‚Äî √Årvore de Decis√£o
```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)

plt.figure(figsize=(12,6))
plot_tree(tree, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
```

### Vantagens
- F√°cil interpreta√ß√£o.  
- Visualiza√ß√£o intuitiva.  

### Limita√ß√µes
- Tendem a **overfitting**.  
- Pequenas mudan√ßas nos dados podem alterar muito a estrutura.  

---

## 6.4 Florestas Aleat√≥rias (Random Forests)

### Conceito
As **Random Forests** s√£o um m√©todo de **ensemble** (conjunto de modelos) baseado em √Årvores de Decis√£o.  
A ideia √© que, em vez de treinar uma √∫nica √°rvore (que pode superajustar os dados), treinamos **muitas √°rvores em paralelo** e deixamos que elas ‚Äúvotem‚Äù na decis√£o final.

Principais conceitos:
- **Bagging (Bootstrap Aggregating):** cada √°rvore √© treinada em uma amostra aleat√≥ria com reposi√ß√£o dos dados.  
- **Sele√ß√£o aleat√≥ria de features:** cada divis√£o dentro da √°rvore considera apenas um subconjunto de vari√°veis.  
- **Vota√ß√£o majorit√°ria:** a classe predita √© a mais votada entre todas as √°rvores.  

üîπ Benef√≠cios:
- Reduz overfitting em compara√ß√£o a √°rvores individuais.  
- √â robusta mesmo quando parte dos dados √© ruidosa.  
- Mede a **import√¢ncia de cada vari√°vel** (feature importance), o que √© muito √∫til em estudos biom√©dicos para indicar quais fatores cl√≠nicos/gen√©ticos influenciam mais a predi√ß√£o.

üî¨ **Exemplo biom√©dico**:  
Queremos prever risco de infarto a partir de vari√°veis cl√≠nicas (idade, IMC, colesterol, press√£o arterial, hist√≥rico familiar).  
- Uma √°rvore isolada pode tomar decis√µes enviesadas (ex.: dar peso exagerado apenas √† idade).  
- A Random Forest combina **centenas de √°rvores**, e o consenso resulta em maior robustez e generaliza√ß√£o.

### C√≥digo ‚Äî Random Forest
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print(classification_report(y_test, y_pred_rf, target_names=iris.target_names))
```

### Vantagens
- Reduz overfitting.  
- Mede a import√¢ncia das vari√°veis.  

### Limita√ß√µes
- Menos interpret√°vel que uma √∫nica √°rvore.  
- Mais custosa computacionalmente.  

---

## 6.5 Compara√ß√£o entre modelos

### M√©tricas comuns
- **Acur√°cia**: % de acertos.  
- **Precis√£o**: propor√ß√£o de positivos corretos.  
- **Recall (sensibilidade)**: propor√ß√£o de positivos detectados.  
- **F1-score**: harm√¥nico entre precis√£o e recall.  
- **ROC-AUC**: separa√ß√£o global entre classes.  

```python
from sklearn.metrics import accuracy_score

print("Acur√°cia SVM:", accuracy_score(y_test, y_pred))
print("Acur√°cia Random Forest:", accuracy_score(y_test, y_pred_rf))
```

---

## 6.6 Boas pr√°ticas em classifica√ß√£o
- **Balanceamento de classes:** use *stratified split* ou t√©cnicas de oversampling/undersampling.  
- **Cross-validation:** prefer√≠vel ao split √∫nico para avalia√ß√£o robusta.  
- **Pr√©-processamento:** padronizar vari√°veis cont√≠nuas para SVM e regress√µes.  
- **Documenta√ß√£o:** registre hiperpar√¢metros usados e semente aleat√≥ria.  

---

## 6.7 Exerc√≠cios Integradores

1. Treine uma SVM com kernel linear no dataset Iris e compare com o kernel RBF.  
2. Construa uma √°rvore de decis√£o sem limitar profundidade. O que acontece?  
3. Treine uma Random Forest com 500 √°rvores e compare desempenho.  
4. Gere a **import√¢ncia das features** em uma Random Forest e interprete.  
5. Simule um dataset cl√≠nico com idade, IMC, glicemia e risco de diabetes, e aplique uma √°rvore.  
6. Aplique valida√ß√£o cruzada (k-fold) a um modelo SVM.  
7. Compare precis√£o, recall e F1-score para cada classe do Iris em SVM e Random Forest.  
8. Plote a matriz de confus√£o para o melhor modelo.  
9. Treine um classificador no dataset de **diabetes** (`sklearn.datasets.load_diabetes`).  
10. Escreva um relat√≥rio explicando qual modelo seria mais confi√°vel para um estudo cl√≠nico, justificando.  

---

## 6.8 Conclus√£o
Neste cap√≠tulo, vimos tr√™s modelos fundamentais de classifica√ß√£o:
- **SVMs:** poderosas em alta dimensionalidade, mas menos interpret√°veis.  
- **√Årvores de Decis√£o:** simples e interpret√°veis, mas propensas a overfitting.  
- **Random Forests:** equil√≠brio entre robustez e desempenho.  

A escolha do modelo depende do problema, do tamanho da base e da necessidade de interpretabilidade.
---

## üîô Navega√ß√£o
- ‚¨ÖÔ∏è [Voltar ao Sum√°rio do Curso](../README.md)
